name: Backfill event-level dataset for a past date

# Prevent overlapping backfills from canceling each other
concurrency:
  group: backfill-${{ github.ref }}-${{ inputs.backfill_date }}
  cancel-in-progress: false

on:
  workflow_dispatch:
    inputs:
      backfill_date:
        description: "Service date (YYYY-MM-DD)"
        required: true
        type: string
      artifact_name:
        description: "Name of the realtime CSVs artifact from your main workflow"
        required: true
        default: "subway_realtime_csvs"
        type: string
      source_workflow_filename:
        description: "Your main pipeline workflow filename in .github/workflows/"
        required: true
        default: "mta.yml"  # change at run time if yours is pipeline.yml
        type: string
      transfer_window_min:
        description: "Transfer window minutes"
        required: true
        default: "20"
        type: string
      missed_threshold_min:
        description: "Missed transfer threshold minutes"
        required: true
        default: "2"
        type: string

jobs:
  backfill:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    permissions:
      contents: read
      actions: read
    env:
      # make the secret available as an env var for if: guards
      RCLONE_CONF_B64: ${{ secrets.RCLONE_CONF_B64 }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install deps
        run: pip install -r requirements.txt

      - name: Install gh/jq/unzip
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq unzip
          echo "${{ github.token }}" | gh auth login --with-token

      - name: Resolve inputs
        id: vars
        run: |
          echo "DATE=${{ inputs.backfill_date }}" >> $GITHUB_OUTPUT
          echo "ART=${{ inputs.artifact_name }}" >> $GITHUB_OUTPUT
          echo "WF_FILE=${{ inputs.source_workflow_filename }}" >> $GITHUB_OUTPUT
          echo "START=${{ inputs.backfill_date }}T00:00:00Z" >> $GITHUB_OUTPUT
          echo "END=${{ inputs.backfill_date }}T23:59:59Z" >> $GITHUB_OUTPUT

      - name: Find workflow id
        id: wf
        run: |
          WF_FILE="${{ steps.vars.outputs.WF_FILE }}"
          WF_ID=$(gh api /repos/${{ github.repository }}/actions/workflows \
            --jq ".workflows[] | select(.path | endswith(\"/${WF_FILE}\")) | .id")
          if [ -z "$WF_ID" ]; then
            echo "Could not find workflow $WF_FILE"; exit 1; fi
          echo "WF_ID=$WF_ID" >> $GITHUB_OUTPUT

      - name: List runs on that date
        id: runs
        run: |
          WF_ID="${{ steps.wf.outputs.WF_ID }}"
          gh api "/repos/${{ github.repository }}/actions/workflows/${WF_ID}/runs?per_page=200" \
            --jq ".workflow_runs[] | {id, created_at}" > runs.json
          jq --arg start "${{ steps.vars.outputs.START }}" --arg end "${{ steps.vars.outputs.END }}" \
             'select(.created_at >= $start and .created_at <= $end)' runs.json > runs_filtered.json
          jq -s '[.[] | .id]' runs_filtered.json > run_ids.json
          COUNT=$(jq length run_ids.json)
          echo "COUNT=$COUNT" >> $GITHUB_OUTPUT
          echo "Found $COUNT runs"

      - name: Download & merge realtime artifacts
        if: steps.runs.outputs.COUNT != '0'
        timeout-minutes: 15
        run: |
          DATE="${{ steps.vars.outputs.DATE }}"
          ART="${{ steps.vars.outputs.ART }}"
          mkdir -p "data/realtime/${DATE}"
          for RID in $(jq -r '.[]' run_ids.json); do
            gh api "/repos/${{ github.repository }}/actions/runs/${RID}/artifacts" > "artifacts_${RID}.json"
            for AID in $(jq -r --arg n "$ART" '.artifacts[] | select(.name==$n) | .id' "artifacts_${RID}.json"); do
              gh api "/repos/${{ github.repository }}/actions/artifacts/${AID}/zip" > "artifact_${RID}_${AID}.zip"
              unzip -o "artifact_${RID}_${AID}.zip" -d "data/realtime/${DATE}/"
            done
          done
          echo "Merged files in data/realtime/${DATE}:"
          ls -lh "data/realtime/${DATE}" || true

      - name: Fail if no CSVs recovered
        run: |
          DATE="${{ steps.vars.outputs.DATE }}"
          ls "data/realtime/${DATE}"/*.csv 1>/dev/null 2>&1 || { echo "No CSVs found"; exit 1; }

      # Cache + prime static GTFS so we don't re-download every run
      - name: Cache GTFS static
        id: gtfs_cache
        uses: actions/cache@v4
        with:
          path: gtfs_static
          key: gtfs-static-v1
          restore-keys: |
            gtfs-static-

      - name: Show cache status & list gtfs_static
        run: |
          echo "cache-hit=${{ steps.gtfs_cache.outputs.cache-hit }}"
          ls -lh gtfs_static || true
          ls -lh gtfs_static/*.zip 2>/dev/null || echo "No static zips yet."

      - name: Prime static GTFS if missing
        timeout-minutes: 10
        run: |
          if ls gtfs_static/subway_all_*.zip 1>/dev/null 2>&1; then
            echo "Static GTFS already present. Skipping refresh."
          else
            echo "No static GTFS found. Refreshing…"
            python pollers/mta_static_refresh.py
          fi

      - name: Run build_master once on the merged folder
        timeout-minutes: 40
        env:
          PYTHONUNBUFFERED: "1"             # stream Python logs live
          ALLOW_STATIC_REFRESH: "0"         # fail fast if static is missing
          RT_DIR: data/realtime/${{ steps.vars.outputs.DATE }}
          RT_MAX_FILES: "999999"
          SERVICE_DATE: ${{ steps.vars.outputs.DATE }}
          TRANSFER_WINDOW_MIN: ${{ inputs.transfer_window_min }}
          MISSED_THRESHOLD_MIN: ${{ inputs.missed_threshold_min }}
        run: |
          set -euo pipefai
          echo "RT files:"; ls -lh "data/realtime/${{ steps.vars.outputs.DATE }}" | sed -n '1,80p'
          trap 'echo "::warning:: Received SIGTERM at $(date -u). Runner likely canceled the step."' TERM
          echo "Starting build_master…"
          python pollers/build_master.py

      - name: Upload curated partition as artifact
        timeout-minutes: 10
        uses: actions/upload-artifact@v4
        with:
          name: curated_backfill_${{ steps.vars.outputs.DATE }}
          path: |
            data/curated/master_interface_dataset.csv
            data/curated/master_interface_dataset.parquet
            data/curated/master_interface_agg.csv
            data/curated/master_interface_agg.parquet
            data/curated/master_interface_dataset_summary.json
            data/curated/service_date=${{ steps.vars.outputs.DATE }}/**
          retention-days: 30

      # Optional: push curated history to Google Drive
      - name: Install rclone
        run: curl -fsSL https://rclone.org/install.sh | sudo bash

      - name: Write rclone.conf
        if: ${{ env.RCLONE_CONF_B64 != '' }}
        run: |
          mkdir -p ~/.config/rclone
          echo "${RCLONE_CONF_B64}" | base64 -d > ~/.config/rclone/rclone.conf
          chmod 600 ~/.config/rclone/rclone.conf

      - name: Sync curated to Drive (optional)
        if: ${{ env.RCLONE_CONF_B64 != '' }}
        timeout-minutes: 15
        run: |
          rclone copy data/curated googledrive:penn-station/curated \
            --create-empty-src-dirs --checksum --transfers=4 --checkers=8
