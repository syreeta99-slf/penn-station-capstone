name: MTA Subway – Rolling Master to Google Drive

on:
  schedule:
    - cron: "*/10 10-15 * * 1-5"
    - cron: "*/10 20-23 * * 1-5"
    - cron: "*/10 0-1 * * 1-5"
    - cron: "0 12-23 * * 6,0"
    - cron: "0 0-2 * * 0,1"
  workflow_dispatch:
    inputs:
      missed_threshold_min:
        description: "Minutes needed to make a transfer (missed if gap < this)"
        required: false
        type: choice
        default: "2"
        options: ["1","2","3","5"]
      transfer_window_min:
        description: "Max window to consider a transfer (minutes)"
        required: false
        type: choice
        default: "20"
        options: ["10","15","20","25","30"]
      rt_max_files:
        description: "How many most-recent RT CSVs to consider"
        required: false
        default: "120"
        type: choice
        options: ["60","90","120","180","240"]
      service_date:
        description: "Override service date (YYYY-MM-DD); leave blank to infer"
        required: false
        default: ""
        type: string

concurrency:
  group: pipeline-${{ github.ref }}
  cancel-in-progress: false

jobs:
  subway_pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read
      actions: read

    env:
      # --- Google Drive sync target (your existing values) ---
      GDRIVE_REMOTE_NAME: "googledrive"
      GDRIVE_DIR: "penn-station/curated"

      # --- Realtime input folders (used by build_master.py) ---
      MTA_RT_DIR: data/realtime          # your subway poller writes here already
      NJT_RT_DIR: data/njt_rt            # NJT poller will write here

      # --- Output master path (used by the script internally as default too) ---
      MASTER_OUT: data/curated/master_interface_dataset.csv

      # --- Interface pairing controls (can be overridden via workflow_dispatch) ---
      TOL_MIN: "15"
      TRANSFER_WINDOW_MIN: "20"
      MISSED_THRESHOLD_MIN: "2"

      # --- Optional filters/overrides ---
      # PENN_STOP_IDS: "A28,A27,PSNY"     # restrict to Penn by stop_ids if you want
      # SERVICE_DATE: "2025-10-06"        # or let the script infer

      # --- NJT API host (creds come from repo secrets in the step) ---
      NJT_ENV: "prod"
      NJT_BASE: "https://raildata.njtransit.com"

    steps:
      - uses: actions/checkout@v4
      
      - name: Ensure gtfs_static exists
        run: mkdir -p gtfs_static

      - uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Sanity check repo layout
        run: |
          echo "[repo] top-level:"; ls -lh
          test -f pollers/mta_realtime_subway.py || { echo "::error::pollers/mta_realtime_subway.py missing"; exit 1; }
          test -f pollers/build_master.py        || { echo "::error::pollers/build_master.py missing"; exit 1; }
          test -f pollers/njt_realtime_rail.py   || { echo "::error::pollers/njt_realtime_rail.py missing"; exit 1; }

      # ===== rclone install & configure =====
      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash
          rclone version

      - name: Write rclone.conf from Base64 secret
        run: |
          mkdir -p ~/.config/rclone
          echo "${{ secrets.RCLONE_CONF_B64 }}" | base64 -d > ~/.config/rclone/rclone.conf
          chmod 600 ~/.config/rclone/rclone.conf
          rclone listremotes
          rclone about "${{ env.GDRIVE_REMOTE_NAME }}:" || (echo "Drive remote not reachable"; exit 1)

      # ===== PRE-SYNC: pull current master from Drive so we can append =====
      - name: Pull prior master from Drive
        run: |
          mkdir -p data/curated
          rclone lsd "${{ env.GDRIVE_REMOTE_NAME }}:${{ env.GDRIVE_DIR }}" || true
          rclone copy "${{ env.GDRIVE_REMOTE_NAME }}:${{ env.GDRIVE_DIR }}" data/curated \
            --include "master_interface_dataset.csv" \
            --include "master_interface_dataset.parquet" \
            --max-depth 1 --ignore-existing || true
          ls -lh data/curated || true

      # ===== Poll realtime (MTA Subway) =====
      - name: Run MTA realtime poller
        env:
          MTA_API_KEY: ${{ secrets.MTA_API_KEY }}
        run: python pollers/mta_realtime_subway.py

      # ===== Poll realtime (NJT Rail) – normalized CSVs =====
      - name: Compute UTC date key
        id: date
        run: echo "today=$(date -u +%F)" >> $GITHUB_OUTPUT

      - name: Cache NJT token (per UTC day)
        uses: actions/cache@v4
        with:
          path: ~/.njt/token.json
          key: njt-token-${{ steps.date.outputs.today }}

      - name: Run NJT realtime poller
        env:
          NJT_CLIENT_ID: ${{ secrets.NJT_CLIENT_ID }}
          NJT_CLIENT_SECRET: ${{ secrets.NJT_CLIENT_SECRET }}
          NJT_TOKEN_DIR: ~/.njt
          NJT_RT_DIR: ${{ env.NJT_RT_DIR }}
          NJT_ENV: ${{ env.NJT_ENV }}
          NJT_BASE: ${{ env.NJT_BASE }}
        run: |
          mkdir -p "${{ env.NJT_RT_DIR }}"
          python pollers/njt_realtime_rail.py || true   # non-fatal if daily token limit

      - name: Show NJT poller logs & directory
        run: |
          echo "NJT_RT_DIR=${{ env.NJT_RT_DIR }}"
          ls -lh "${{ env.NJT_RT_DIR }}" || true
          # Show the 5 most recent files (if any)
          find "${{ env.NJT_RT_DIR }}" -maxdepth 1 -type f -printf "%TY-%Tm-%Td %TT %p\n" | sort | tail -n 5 || true


      - name: Upload realtime CSVs (artifact)
        uses: actions/upload-artifact@v4
        with:
          name: realtime_csvs
          path: |
            data/realtime/*.csv
            data/njt_rt/*.csv
          retention-days: 3

      # ===== Ensure static GTFS present =====
      - name: Cache GTFS static
        id: gtfs_cache
        uses: actions/cache@v4
        with:
          path: gtfs_static
          key: gtfs-static-v1
          restore-keys: gtfs-static-

      - name: Prime static GTFS if missing
        timeout-minutes: 10
        run: |
          if ls gtfs_static/subway_all_*.zip 1>/dev/null 2>&1; then
            echo "Static GTFS present."
          else
            echo "Refreshing static GTFS…"
            python pollers/mta_static_refresh.py
          fi


      - name: Show MTA+NJT input counts before build
        run: |
          echo "[debug] MTA_RT_DIR=${{ env.MTA_RT_DIR }}"
          echo "[debug] NJT_RT_DIR=${{ env.NJT_RT_DIR }}"
          echo "[debug] MTA files:"; ls -lh "${{ env.MTA_RT_DIR }}" || true
          echo "[debug] NJT files:"; ls -lh "${{ env.NJT_RT_DIR }}" || true

      # ===== Build & APPEND to rolling master (now reads MTA + NJT RT) =====
      - name: Build master & append
        env:
          PYTHONUNBUFFERED: "1"
          APPEND_TO_MASTER: "1"
          ALLOW_STATIC_REFRESH: "0"
          # Paths and tuning (flow to build_master.py)
          MTA_RT_DIR: ${{ env.MTA_RT_DIR }}
          NJT_RT_DIR: ${{ env.NJT_RT_DIR }}
          MASTER_OUT: ${{ env.MASTER_OUT }}
          TOL_MIN: ${{ env.TOL_MIN }}
          TRANSFER_WINDOW_MIN: ${{ inputs.transfer_window_min || env.TRANSFER_WINDOW_MIN }}
          MISSED_THRESHOLD_MIN: ${{ inputs.missed_threshold_min || env.MISSED_THRESHOLD_MIN }}
          SERVICE_DATE: ${{ inputs.service_date || '' }}
          # Optional: PENN_STOP_IDS: ${{ env.PENN_STOP_IDS }}
        run: |
          echo "[t0] starting build_master"
          python pollers/build_master.py
          echo "[t1] build_master finished"

      - name: Upload curated outputs (artifact)
        uses: actions/upload-artifact@v4
        with:
          name: curated_outputs
          path: |
            data/curated/master_interface_dataset.csv
            data/curated/master_interface_dataset.parquet
          retention-days: 7

      # ===== POST-SYNC: push updated master back to Drive =====
      - name: Push updated master to Drive
        run: |
          rclone copy data/curated "${{ env.GDRIVE_REMOTE_NAME }}:${{ env.GDRIVE_DIR }}" \
            --include "master_interface_dataset.csv" \
            --include "master_interface_dataset.parquet" \
            --create-empty-src-dirs \
            --transfers 4 --checkers 8 \
            --drive-chunk-size 64M \
            --update --progress
