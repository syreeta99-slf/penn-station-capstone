name: MTA Subway Static + Realtime + Google Drive

on:
  schedule:
    # Weekday peaks (America/New_York) with UTC buffers, every 10 minutes
    - cron: "*/10 10-15 * * 1-5"   # AM 06:00–11:59 ET (covers DST switch)
    - cron: "*/10 20-23 * * 1-5"   # PM 16:00–19:59 ET
    - cron: "*/10 0-1 * * 1-5"     # PM 20:00–21:59 ET
    # Weekly static refresh (Sun 03:10 UTC)
    - cron: "10 3 * * 0"
  workflow_dispatch:
    inputs:
      run_static_refresh:
        description: "Run GTFS static refresh now?"
        type: boolean
        default: false

jobs:
  pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: pip install -r requirements.txt

      # Refresh GTFS static weekly (cron) OR when manually requested via the checkbox
      - name: Refresh GTFS static (weekly or manual)
        if: >
          (github.event_name == 'schedule' && github.event.schedule == '10 3 * * 0') ||
          (github.event_name == 'workflow_dispatch' && inputs.run_static_refresh == true)
        run: python pollers/mta_static_refresh.py

      # Pull realtime (protobuf) every scheduled run (and manual runs)
      - name: Poll MTA Subway realtime
        run: python pollers/mta_realtime_subway.py
        # Upload the realtime CSVs so you can download them after the run
        
      - name: Upload realtime CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: subway_realtime_csvs
          path: data/realtime/*.csv
          retention-days: 7

      # Build curated master (uses latest static ZIP + realtime)
      - name: Build Master dataset
        run: python pollers/build_master.py

      # --- Upload to Google Drive via rclone ---

      # Ensure curated folder exists (first run may be empty)
      - name: Ensure curated folder exists
        run: mkdir -p data/curated

      # Install rclone on the runner
      - name: Install rclone
        run: curl -fsSL https://rclone.org/install.sh | sudo bash

      # Decode your Base64 rclone.conf secret into the runner
      - name: Write rclone.conf from Base64 secret
        run: |
          mkdir -p ~/.config/rclone
          echo "${{ secrets.RCLONE_CONF_B64 }}" | base64 -d > ~/.config/rclone/rclone.conf
          chmod 600 ~/.config/rclone/rclone.conf

      # Sanity check: confirm the remote exists and is reachable
      # Replace 'googledrive' if your remote name differs
      - name: Verify rclone remote
        run: |
          rclone listremotes
          rclone lsd googledrive: || (echo "Remote 'googledrive' not found; check your remote name and secret" && exit 1)

      # Copy curated outputs to Google Drive (My Drive/penn-station/curated)
      - name: Upload curated dataset to Google Drive
        run: |
          if [ -z "$(ls -A data/curated || true)" ]; then
            echo "Nothing to upload yet (data/curated is empty)."
          else
            rclone copy data/curated googledrive:penn-station/curated \
              --create-empty-src-dirs --checksum --transfers=4 --checkers=8
          fi

      # Optional: also save curated files as a workflow artifact
      - name: Upload curated artifacts
        uses: actions/upload-artifact@v4
        with:
          name: curated_${{ github.run_id }}
          path: data/curated/*
          retention-days: 30
