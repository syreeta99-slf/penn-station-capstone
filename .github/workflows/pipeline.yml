name: MTA Subway – Scheduled Build

on:
  schedule:
    # UTC schedule; these cover common NYC windows (adjust as you like)
    - cron: "*/10 10-15 * * 1-5"  # Weekday AM coverage (06:00–11:59 ET)
    - cron: "*/10 20-23 * * 1-5"  # Weekday PM coverage (16:00–19:59 ET)
    - cron: "*/10 0-1 * * 1-5"    # Weekday evening (20:00–21:59 ET)
    - cron: "0 12-23 * * 6,0"     # Weekend hourly (08:00–19:00 ET)
    - cron: "0 0-2 * * 0,1"       # Weekend late (20:00–22:59 ET)
  workflow_dispatch:
     inputs:
      missed_threshold_min:
        description: "Minutes needed to make a transfer (missed if gap < this)"
        required: false
        type: choice
        default: "2"
        options: ["1","2","3","5"]
      transfer_window_min:
        description: "Max window to consider a transfer (minutes)"
        required: false
        type: choice
        default: "20"
        options: ["10","15","20","25","30"]
      rt_max_files:
        description: "How many most-recent RT CSVs to consider"
        required: false
        default: "120"
        type: choice
        options: ["60","90","120","180","240"]
      service_date:
        description: "Override service date (YYYY-MM-DD); leave blank to infer"
        required: false
        default: ""
        type: string

concurrency:
  group: pipeline-${{ github.ref }}
  cancel-in-progress: false

jobs:
  subway_pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read
      actions: read

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: pip install -r requirements.txt

      - name: Sanity check repo layout
        run: |
          echo "[repo] top-level:"
          ls -lh
          echo "[pollers/] contents:"
          ls -lh pollers || true
          test -f pollers/mta_realtime_subway.py || { echo "::error::pollers/mta_realtime_subway.py missing"; exit 1; }
          test -f pollers/build_master.py        || { echo "::error::pollers/build_master.py missing"; exit 1; }

      # ---- Poll realtime (writes data/realtime/subway_rt_YYYYmmdd_HHMMSS.csv)
      - name: Run realtime poller
        env:
          MTA_API_KEY: ${{ secrets.MTA_API_KEY }}
        run: python pollers/mta_realtime_subway.py

      # Keep a short-lived artifact of the raw CSVs for backfill/debug
      - name: Upload realtime CSVs (artifact)
        uses: actions/upload-artifact@v4
        with:
          name: subway_realtime_csvs
          path: data/realtime/*.csv
          retention-days: 3

      # ---- Cache GTFS static so build_master won’t refetch every time
      - name: Cache GTFS static
        id: gtfs_cache
        uses: actions/cache@v4
        with:
          path: gtfs_static
          key: gtfs-static-v1
          restore-keys: gtfs-static-

      - name: Prime static GTFS if missing
        timeout-minutes: 10
        run: |
          if ls gtfs_static/subway_all_*.zip 1>/dev/null 2>&1; then
            echo "Static GTFS present."
          else
            echo "Refreshing static GTFS…"
            python pollers/mta_static_refresh.py
          fi

      # ---- Build & append to rolling master
      - name: Build master & append
        env:
          PYTHONUNBUFFERED: "1"
          APPEND_TO_MASTER: "1"
          ALLOW_STATIC_REFRESH: "0"
          RT_DIR: data/realtime
          RT_MAX_FILES: ${{ inputs.rt_max_files || '120' }}
          TRANSFER_WINDOW_MIN: ${{ inputs.transfer_window_min || '20' }}
          MISSED_THRESHOLD_MIN: ${{ inputs.missed_threshold_min || '2' }}
          SERVICE_DATE: ${{ inputs.service_date || '' }}
        run: |
          echo "[t0] starting build_master"
          python pollers/build_master.py
          echo "[t1] build_master finished"
          
      # ---- Upload curated outputs (what you analyze / sync to Drive)
      - name: Upload curated outputs
        uses: actions/upload-artifact@v4
        with:
          name: curated_outputs
          path: |
            data/curated/master_interface_dataset.csv
            data/curated/master_interface_dataset.parquet
            data/curated/master_interface_agg.csv
            data/curated/master_interface_agg.parquet
            data/curated/master_interface_dataset_summary.json
          retention-days: 7

      # If you sync to Google Drive in this pipeline, keep your existing rclone step here.
      # Example (uncomment & set your remote + path):
      # - name: Sync curated to Google Drive
      #   if: ${{ secrets.RCLONE_CONF_B64 != '' }}
      #   env:
      #     RCLONE_CONF_B64: ${{ secrets.RCLONE_CONF_B64 }}
      #   run: |
      #     mkdir -p ~/.config/rclone
      #     echo "$RCLONE_CONF_B64" | base64 -d > ~/.config/rclone/rclone.conf
      #     chmod 600 ~/.config/rclone/rclone.conf
      #     rclone copy data/curated googledrive:penn-station/curated \
      #       --create-empty-src-dirs --checksum --transfers=4 --checkers=8
