name: MTA Subway Static + Realtime + Google Drive

on:
  schedule:
    # Weekday peaks (America/New_York) with UTC buffers, every 10 minutes
    - cron: "*/10 10-15 * * 1-5"   # AM 06:00–11:59 ET (covers DST switch)
    - cron: "*/10 20-23 * * 1-5"   # PM 16:00–19:59 ET
    - cron: "*/10 0-1 * * 1-5"     # PM 20:00–21:59 ET
    # Weekend hourly coverage
    - cron: "0 12-23 * * 6,0"   # Sat/Sun 08:00–19:00 ET
    - cron: "0 0-2 * * 0,1"     # Sat/Sun 20:00–22:59 ET
    - cron: "0 3-3 * * 0"         # 23:00 ET Sunday overlap (optional tidy-up hour)
    # Weekly static refresh (Sun 03:10 UTC)
    - cron: "10 3 * * 0"
    
  workflow_dispatch:
    inputs:
      run_static_refresh:
        description: "Run GTFS static refresh now?"
        type: boolean
        default: false
      transfer_window_min:
        description: "Transfer window (minutes)"
        required: true
        default: "20"
      missed_threshold_min:
        description: "Missed threshold (minutes)"
        required: true
        default: "2"

jobs:
  pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -r requirements.txt

      # Resolve thresholds for BOTH cron and manual runs (with defaults)
      - name: Resolve threshold env vars
        run: |
          TW="${{ github.event.inputs.transfer_window_min }}"
          MT="${{ github.event.inputs.missed_threshold_min }}"
          # Defaults if inputs are empty (e.g., on scheduled runs)
          [ -z "$TW" ] && TW=20
          [ -z "$MT" ] && MT=2
          echo "TRANSFER_WINDOW_MIN=$TW" >> "$GITHUB_ENV"
          echo "MISSED_THRESHOLD_MIN=$MT" >> "$GITHUB_ENV"
          echo "Resolved TRANSFER_WINDOW_MIN=$TW, MISSED_THRESHOLD_MIN=$MT"

      # Refresh GTFS static weekly (cron) OR when manually requested via the checkbox
      - name: Refresh GTFS static (weekly or manual)
        if: >
          (github.event_name == 'schedule' && github.event.schedule == '10 3 * * 0') ||
          (github.event_name == 'workflow_dispatch' && inputs.run_static_refresh == true)
        run: python pollers/mta_static_refresh.py

      # Ensure at least one static ZIP exists (auto-run refresh if missing)
      - name: Ensure static GTFS exists
        run: |
          set -e
          if ls gtfs_static/subway_all_*.zip 1> /dev/null 2>&1; then
            echo "Static GTFS found:"
            ls -lh gtfs_static/subway_all_*.zip | tail -n 3
          else
            echo "No static GTFS found. Running static refresh..."
            python pollers/mta_static_refresh.py
            echo "Downloaded static:"
            ls -lh gtfs_static
          fi

      # Pull realtime (protobuf) every run (creates data/realtime/subway_rt_*.csv)
      - name: Poll MTA Subway realtime
        run: |
          mkdir -p data/realtime
          python pollers/mta_realtime_subway.py
          echo "Latest realtime CSVs:"
          ls -lh data/realtime | tail -n 5

      # Optional: peek at latest realtime stop_ids (diagnostic)
      - name: Debug unique stop_ids
        run: |
          python - <<'EOF'
          import pandas as pd, glob
          files = sorted(glob.glob("data/realtime/*.csv"))
          if files:
              df = pd.read_csv(files[-1])
              print("Unique stop_ids in latest CSV:", sorted(df["stop_id"].dropna().unique().tolist())[:25])
              print("Example rows:\n", df.head(5))
          else:
              print("No realtime CSVs found.")
          EOF

      # Build curated master (uses latest static ZIP + realtime)
      # Picks up TRANSFER_WINDOW_MIN and MISSED_THRESHOLD_MIN from $GITHUB_ENV
      - name: Build Master dataset
        run: python pollers/build_master.py

      - name: Upload debug joined events (artifact)
        uses: actions/upload-artifact@v4
        with:
          name: debug_events_at_penn
          path: data/curated/_debug_events_at_penn.csv
          retention-days: 5

      # Upload the realtime CSVs for inspection
      - name: Upload realtime CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: subway_realtime_csvs
          path: data/realtime/*.csv
          retention-days: 7

      - name: Upload master dataset artifact
        uses: actions/upload-artifact@v4
        with:
          name: master_interface_dataset
          path: |
            data/curated/master_interface_dataset.csv
            data/curated/master_interface_dataset.parquet
            data/curated/master_interface_dataset_summary.json
          retention-days: 14

      # --- Upload to Google Drive via rclone ---
      - name: Ensure curated folder exists
        run: mkdir -p data/curated

      - name: Install rclone
        run: curl -fsSL https://rclone.org/install.sh | sudo bash

      - name: Write rclone.conf from Base64 secret
        run: |
          mkdir -p ~/.config/rclone
          echo "${{ secrets.RCLONE_CONF_B64 }}" | base64 -d > ~/.config/rclone/rclone.conf
          chmod 600 ~/.config/rclone/rclone.conf

      - name: Verify rclone remote
        run: |
          rclone listremotes
          rclone lsd googledrive: || (echo "Remote 'googledrive' not found; check your remote name and secret" && exit 1)

      - name: Upload curated dataset to Google Drive
        run: |
          if [ -z "$(ls -A data/curated || true)" ]; then
            echo "Nothing to upload yet (data/curated is empty)."
          else
            rclone copy data/curated googledrive:penn-station/curated \
              --create-empty-src-dirs --checksum --transfers=4 --checkers=8
          fi

      - name: Upload curated artifacts (all)
        uses: actions/upload-artifact@v4
        with:
          name: curated_${{ github.run_id }}
          path: data/curated/*
          retention-days: 30
